par(mfrow=c(2,2))
plot(Mass_LinReg1)
Mass_LinReg1
par(mfrow=c(2,2))
plot(LinReg1_Mass)
plot(LinReg1_Mass,which=4)
head(train)
par(mfrow=c(2,2))
plot(Mass_LinReg1)
plot(Mass_LinReg1)
Mass_LinReg1<- lm( TotalRevenueGenerated ~ ., data=train)
par(mfrow=c(2,2))
plot(Mass_LinReg1)
#Review the residual plots
par(mfrow=c(1,1))
plot(Mass_LinReg1)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train)%in%c(???))
#Error verification on train data
regr.eval(train$TotalRevenueGenerated, Mass_LinReg1$fitted.values)
#Error verification on test data
MASS_Pred1<-predict(Mass_LinReg1,test)
regr.eval(test$TotalRevenueGenerated, MASS_Pred1)
Error_calc = data.frame(train$TotalRevenueGenerated,Mass_LinReg1$fitted.values)
write.csv(x = Error_calc,file = "Error_calc.csv")
Error_calc = data.frame(TotalRevenue = train$TotalRevenueGenerated,Fitted_Val = Mass_LinReg1$fitted.values)
write.csv(x = Error_calc,file = "Error_calc.csv")
which(rownames(train)%in%c(???))
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
Mass_LinReg1.which(rownames(train)%in%c(???))
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
Mass_LinReg1[Mass_LinReg1.which(rownames(train)%in%c(???))]
#Exp2- Variable Transformations Y and/or X variables
#Y Variable Transformation - Apply log
datatemp <- data
View(datatemp)
# select the final list of variables
Mass_data <- subset(dataForModel, select=-c(MaxAgeOfChild,NoOfGamesPlayed))
#DIY split the data into train and test data sets (70/30 Split)
rows = seq(1,nrow(Mass_data),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(Mass_data))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train <- Mass_data[trainRows,]
test <- Mass_data[-trainRows,]
Mass_LinReg1<- lm( TotalRevenueGenerated ~ ., data=train)
summary(Mass_LinReg1)
#Review the residual plots
par(mfrow=c(1,1))
par(mfrow=c(2,2))
#Error verification on train data
regr.eval(train$TotalRevenueGenerated, Mass_LinReg1$fitted.values)
#Error verification on test data
MASS_Pred1<-predict(Mass_LinReg1,test)
regr.eval(test$TotalRevenueGenerated, MASS_Pred1)
Error_calc = data.frame(TotalRevenue = train$TotalRevenueGenerated,Fitted_Val = Mass_LinReg1$fitted.values)
write.csv(x = Error_calc,file = "Error_calc.csv")
write.csv(x = Error_calc,file = "Error_calc.csv")
#Exp2- Variable Transformations Y and/or X variables
#Y Variable Transformation - Apply log
datatemp <- dataForModel
datatemp$TotalRevenueGenerated <- log(datatemp$TotalRevenueGenerated)
View(dataForModel)
# Build the model and compute the error metrics
rows = seq(1,nrow(datatemp),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(datatemp))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train <- datatemp[trainRows,]
test <- datatemp[-trainRows,]
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train_temp <- datatemp[trainRows,]
test_temp <- datatemp[-trainRows,]
LinReg_transform<- lm( TotalRevenueGenerated ~ ., data=train_temp)
summary(LinReg_transform)
#Review the residual plots
par(mfrow=c(2,2))
plot(LinReg1)
plot(LinReg_transform)
library(e1071)
library(caret) # for box-cox transformation
library(lmtest) # bptest for testing heteroscedasticity
library(gvlma) # global validation of linear model assumptions
#DIY Set directory and read the data
setwd("E://Insofe//Week6//Day1//Lab//20180106_Batch37_CSE7302c_Multiple_Lin_Reg_Lab02//exercise//")
data<-read.csv("CustomerData.csv",header=T)
#DIY Data Exploration - Check Data Structure, and Summary
str(data)
#DIY remove CustomerID Column
data=subset(data,select = -c(CustomerID))
#DIY convert City attribute as factor
data$City=as.factor(data$City)
#DIY split the data into train and test data sets (70/30 Split)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
rows = seq(1, nrow(data),1)
set.seed(123)
trainrows = sample(rows,(70*nrow(data))/100)
data_train = data[trainrows,]
data_test = data[-trainrows,]
# BUILD LINEAR REGRESSION MODEL
LinReg1 =lm(formula =TotalRevenueGenerated~., data = data_train)
# Build model with all attributes into model.
# "TotalRevenueGenerated" is the target variable
summary(LinReg1)
#Review the residual plots
par(mfrow=c(2,2))
plot(LinReg1)
table(resid)
gvlma(x=LinReg1)
library(olsrr)
ols_bp_test(LinReg1)
lmtest::bptest(LinReg1,studentize = F)
plot(LinReg1,which=4)
par(mfrow=c(1,1)) #reset default
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train_mass <- Mass_data[trainRows,]
test_mass <- Mass_data[-trainRows,]
Mass_LinReg1<- lm( TotalRevenueGenerated ~ ., data=train_mass)
summary(Mass_LinReg1)
par(mfrow=c(2,2))
plot(Mass_LinReg1)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
data2=train_mass[-c(974,1764,2729),]
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
data2_train=train_mass[-c(974,1764,2729),]
View(data2_train)
Mass_LinReg2<- lm( TotalRevenueGenerated ~ ., data=data2_train)
summary(Mass_LinReg2)
plot(Mass_LinReg2)
plot(Mass_LinReg2,which=4)
View(data2_train)
write.csv(data2_train,"data2_train.csv")
################################# Removing the outliers and checking the model ################################
data2=data[-c(974,1764,2729),]
data2$TotalRevenueGenerated=log10(data2$TotalRevenueGenerated)
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(data2$TotalRevenueGenerated, SplitRatio = 0.8)
training_set = subset(data2, split == TRUE)
test_set = subset(data2, split == FALSE)
# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = TotalRevenueGenerated ~ .,
data = training_set)
#Review the residual plots
par(mfrow=c(2,2))
plot(regressor)
resid=residuals(LinReg1)
table(resid)
gvlma(x=LinReg1)
library(olsrr)
ols_bp_test(LinReg1)
lmtest::bptest(LinReg1,studentize = F)
plot(regressor,which=4)
par(mfrow=c(1,1)) #reset default
hist(LinReg1$residuals)
options(scipen = 4)
hist(regressor$residuals)
###############################################################################################
########################## Checking for multicolinartity##################################
#########################################################################################
# Check for multicollinearity
# 1. VIF: (check attributes with high VIF value)
library(car)
vif(LinReg_std1)
str(train_std)
# remove the highly correlated attributes whose value is greter than 10 and
data_3=subset(data_std_final,select=-c(FrquncyOfPurchase,NoOfGamesBought))
#Split the data into train and test
rows= seq(1, nrow(data_3),1)
set.seed(123)
trainRows= sample(rows,(70*nrow(data_3))/100)
train_std_mc = data_3[trainRows,]
test_std_mc = data_3[-trainRows,]
# build the model
LinReg_std2<- lm(formula = TotalRevenueGenerated~.,data = train_std_mc)
summary(LinReg_std2)
vif(LinReg_std2)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_mass)%in%c(2729))
train_aic_inf=train_mass[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
par(mfrow=c(2,2))
plot(LinReg_No_infl)
plot(LinReg_No_infl,which=4)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_std_aic)%in%c(2729))
# 2. Stepwise Regression
library(MASS)
Step1 <- stepAIC(LinReg_std1, direction="backward")
#Step2 <- stepAIC(LinReg1, direction="forward")
Step3 <- stepAIC(LinReg_std1, direction="both")
summary(Step3)
# select the final list of variables
data_4=subset(data_3,select=-c(MaxAgeOfChild,NoOfGamesPlayed))
#Split the data into train and test
rows= seq(1, nrow(data_4),1)
set.seed(123)
trainRows= sample(rows,(70*nrow(data_4))/100)
train_std_aic = data_4[trainRows,]
test_std_aic = data_4[-trainRows,]
# and build the model
Mass_LinReg1 <- lm(TotalRevenueGenerated~.,data = train_std_aic)
summary(Mass_LinReg1)
par(mfrow=c(2,2))
plot(Mass_LinReg1)
plot(Mass_LinReg1,which=4)
par(mfrow=c(1,1))
#Error verification on train data
train_std_aic_error=data.frame(regr.eval(train_std_aic$TotalRevenueGenerated, Mass_LinReg1$fitted.values))
colnames(train_std_aic_error)="train_std_aic_error"
#Error verification on test data
target=test_std_aic$TotalRevenueGenerated
test=subset(test_std_aic,select=-c(TotalRevenueGenerated))
pred=predict(Mass_LinReg1,test)
test_std_aic_error=data.frame(regr.eval(target,pred))
colnames(test_std_aic_error)="test_std_aic_error"
std_err_aic_df=data.frame(train_std_aic_error,test_std_aic_error)
error=data.frame(error,std_err_aic_df)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_std_aic)%in%c(2729))
train_std_aic_inf=train_std_aic[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_std_aic_inf)
summary(LinReg_No_infl)
plot(LinReg_No_infl,which=4)
rm(list = ls(all = TRUE))
#DIY Set directory and read the data
setwd(
"E://Insofe//Week6//Day1//Lab//20180106_Batch37_CSE7302c_Multiple_Lin_Reg_Lab02//exercise"
)
data <- read.csv("CustomerData.csv", header = T)
#DIY Data Exploration - Check Data Structure, and Summary
str(data)
summary(data)
#DIY remove CustomerID Column
data <- subset(data, select=-c(CustomerID))
dataForModel = data
#DIY convert City attribute as factor
data$City <- as.factor(data$City)
#DIY split the data into train and test data sets (70/30 Split)
rows = seq(1,nrow(dataForModel),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(dataForModel))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train <- dataForModel[trainRows,]
test <- dataForModel[-trainRows,]
LinReg1<- lm( TotalRevenueGenerated ~ ., data=train)
summary(LinReg1)
#Review the residual plots
par(mfrow=c(2,2))
plot(LinReg1)
plot(LinReg1,which=4)
par(mfrow=c(1,1)) #reset default
# to remove outliers, use cooks distance
hist(LinReg1$residuals)
# Error metrics evaluation on train data and test data
library(DMwR)
#Error verification on train data
regr.eval(train$TotalRevenueGenerated, LinReg1$fitted.values)
#Error verification on test data
target <- test$TotalRevenueGenerated
test$TotalRevenueGenerated <- NULL
test_prediction <- predict(LinReg1,test)
Pred<- regr.eval(target, test_prediction)
Pred
summary(LinReg1)
LinReg2 <- lm(TotalRevenueGenerated ~ City + NoOfChildren + MinAgeOfChild + FrquncyOfPurchase
+ NoOfUnitsPurchased + FrequencyOFPlay + NoOfGamesBought
+ FavoriteChannelOfTransaction + FavoriteGame, data = train)
#In LinReg1 observe the significant values
summary(LinReg2)
par(mfrow=c(2,2))
plot(LinReg2)
regr.eval(train$TotalRevenueGenerated, LinReg2$fitted.values)
test
test_prediction2 <- predict(LinReg2,test)
pred2 <- regr.eval(target,test_prediction2)
pred2
####################################################
#Scaling the data
library(vegan)
str(data)
data_cat <- data[,c(1,11,12)]
data_num <- data[,-c(1,11,12)]
summary(data_num)
# Apply standardization. Ensure , you exclude the target variable
#during standardization
#rm(data_num_subset)
data_num_std <- decostand(subset(data_num, select=-c(TotalRevenueGenerated)),"standardize")
#Combine standardized attributes back with the
# categorical attributes
data_std_final <- cbind(data_num_std,data_cat)
data_std_final <- cbind(data_std_final, data_num$TotalRevenueGenerated)
class(data_std_final)
colnames(data_std_final)[ncol(data_std_final)] <- "TotalRevenueGenerated"
#Split the data into train(70%) and test data sets
rows= seq(1, nrow(data_std_final),1)
set.seed(123)
trainRows= sample(rows,(70*nrow(data_std_final))/100)
train_std = data_std_final[trainRows,]
test_std = data_std_final[-trainRows,]
# Build linear regression with all attributes
LinReg_std1 <- lm(TotalRevenueGenerated~., data=train_std)
#Error verification on train data
regr.eval(train_std$TotalRevenueGenerated,LinReg_std1$fitted.values)
summary(LinReg_std1)
#Error verification on train data
regr.eval(train_std$TotalRevenueGenerated,LinReg_std1$fitted.values)
#Error verification on test data
target_std <- test_std$TotalRevenueGenerated
test_prediction_std1 <- predict(LinReg_std1, test_std)
pred_Std1 <- regr.eval(target_std,test_prediction_std1)
pred_Std1
# 1. VIF: (check attributes with high VIF value)
library(car)
vif(LinReg_std1)
vif(LinReg_std1)
str(train_std)
# remove the highly correlated attributes and
data_3=subset(data_std_final,select=-c(FrquncyOfPurchase,NoOfGamesBought))
#splitting
rows= seq(1, nrow(data_3),1)
set.seed(123)
trainRows= sample(rows,(70*nrow(data_3))/100)
train_std_mc = data_3[trainRows,]
test_std_mc = data_3[-trainRows,]
LinReg_std2<- lm(formula = TotalRevenueGenerated~.,data = train_std_mc)
summary(LinReg_std2)
#summary(LinReg2_std1)
vif(LinReg_std2)
#vif(LinReg2_std1)
#Error verification on test data
train_std_mc_error=data.frame(regr.eval(train_std_mc$TotalRevenueGenerated, LinReg_std2$fitted.values))
colnames(train_std_mc_error)="train_std_error"
train_std_mc_error
#Error verification on test data
target=test_std_mc$TotalRevenueGenerated
test=subset(test_std_mc,select=-c(TotalRevenueGenerated))
pred=predict(LinReg_std2,test)
test_std_mc_error=data.frame(regr.eval(target,pred))
colnames(test_std_mc_error)="test_std_error"
std_err_mc_df=data.frame(train_std_mc_error,test_std_mc_error)
error=data.frame(error,std_err_mc_df)
error=data.frame(error,std_err_mc_df)
regr.eval(target,pred)
# 2. Stepwise Regression
library(MASS)
Step1 <- stepAIC(LinReg_std1, direction="backward")
#Step2 <- stepAIC(LinReg1, direction="forward")
Step3 <- stepAIC(LinReg_std1, direction="both")
summary(Step3)
Step3
# select the final list of variables
Mass_data <- subset(dataForModel, select=-c(MaxAgeOfChild,NoOfGamesPlayed))
#DIY split the data into train and test data sets (70/30 Split)
rows = seq(1,nrow(Mass_data),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(Mass_data))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train_mass <- Mass_data[trainRows,]
test_mass <- Mass_data[-trainRows,]
Mass_LinReg1<- lm( TotalRevenueGenerated ~ ., data=train_mass)
summary(Mass_LinReg1)
#Review the residual plots
par(mfrow=c(1,1))
plot(Mass_LinReg1)
head(train)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_mass)%in%c(2729))
train_aic_inf=train_mass[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
train_aic_inf=train_mass[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
train_mass
# select the final list of variables
Mass_data <- subset(data_std_final, select=-c(MaxAgeOfChild,NoOfGamesPlayed))
#DIY split the data into train and test data sets (70/30 Split)
rows = seq(1,nrow(Mass_data),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(Mass_data))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train_mass <- Mass_data[trainRows,]
test_mass <- Mass_data[-trainRows,]
Mass_LinReg1<- lm( TotalRevenueGenerated ~ ., data=train_mass)
summary(Mass_LinReg1)
#Review the residual plots
par(mfrow=c(1,1))
plot(Mass_LinReg1)
plot(Mass_LinReg1,which=4)
head(train)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_mass)%in%c(2729))
train_aic_inf=train_mass[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
summary(Mass_LinReg1)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_mass)%in%c(2729))
train_aic_inf=train_mass[-c(2729),]
train_aic_inf=train_mass[-c(2729),]
View(train_aic_inf)
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
par(mfrow=c(2,2))
plot(LinReg_No_infl)
train_aic_inf=train_mass[-c(2729),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
par(mfrow=c(2,2))
plot(LinReg_No_infl)
plot(LinReg_No_infl,which=4)
# Identify the outliers using the cook's distance
# remove them
# build model without the influencial points (record #2729)
which(rownames(train_mass)%in%c(2729))
train_aic_inf=train_mass[-c(230),]
train_aic_inf=train_mass[-c(230),]
test_aic_nif=test_mass[-c(230),]
LinReg_No_infl<-lm(TotalRevenueGenerated~.,data = train_aic_inf)
summary(LinReg_No_infl)
par(mfrow=c(2,2))
plot(LinReg_No_infl)
plot(LinReg_No_infl,which=4)
#Error verification on train data
regr.eval(train_aic_inf$TotalRevenueGenerated, LinReg_No_infl$fitted.values)
#Error verification on test data
MASS_Pred1<-predict(Mass_LinReg1,test)
#Error verification on test data
MASS_Pred1<-predict(LinReg_No_infl,test)
#Error verification on test data
MASS_Pred1<-predict(LinReg_No_infl,test_aic_nif)
regr.eval(test$TotalRevenueGenerated, MASS_Pred1)
regr.eval(test_aic_nif$TotalRevenueGenerated, MASS_Pred1)
Error_calc = data.frame(TotalRevenue = train$TotalRevenueGenerated,Fitted_Val = Mass_LinReg1$fitted.values)
write.csv(x = Error_calc,file = "Error_calc.csv")
write.csv(x = Error_calc,file = "Error_calc.csv")
Error_calc = data.frame(TotalRevenue = train_aic_inf$TotalRevenueGenerated,Fitted_Val = LinReg_No_infl$fitted.values)
write.csv(x = Error_calc,file = "Error_calc.csv")
summary(data)
data$MinAgeOfChild=ifelse(data$MinAgeOfChild>100,yes = median(data$MinAgeOfChild),no=data$MinAgeOfChild)
data$MaxAgeOfChild=ifelse(data$MaxAgeOfChild>100,yes = median(data$MaxAgeOfChild),no=data$MaxAgeOfChild)
set.seed(123)
split = sample.split(data$TotalRevenueGenerated, SplitRatio = 0.8)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
View(data)
split = sample.split(data$TotalRevenueGenerated, SplitRatio = 0.8)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
# BUILD LINEAR REGRESSION MODEL
LinearRegression=lm(formula = TotalRevenueGenerated~.,data = training_set)
summary(LinearRegression)
# build model with all attributes into model
par(mfrow=c(2,2))
plot(LinearRegression)
#Error verification on train data
regr.eval(training_set$TotalRevenueGenerated, LinearRegression$fitted.values)
#Error verification on test data
target=test_set$TotalRevenueGenerated
test=subset(test_set,select=-c(TotalRevenueGenerated))
pred=predict(LinearRegression,test)
regr.eval(target,pred)
#Exp2- Variable Transformations Y and/or X variables
#Y Variable Transformation - Apply log
datatemp <- dataForModel
datatemp$TotalRevenueGenerated <- log(datatemp$TotalRevenueGenerated)
# Build the model and compute the error metrics
rows = seq(1,nrow(datatemp),1)
set.seed(123)
trainRows = sample(rows,(70*nrow(datatemp))/100)
#DIY save train data to a dataframe named "train" and test data to a dataframe named "test"
train_temp <- datatemp[trainRows,]
test_temp <- datatemp[-trainRows,]
LinReg_transform<- lm( TotalRevenueGenerated ~ ., data=train_temp)
summary(LinReg_transform)
#Review the residual plots
par(mfrow=c(2,2))
plot(LinReg_transform)
par(mfrow=c(1,1))
#Error verification on train data
regr.eval(train_temp$TotalRevenueGenerated, LinReg_transform$fitted.values)
#Error verification on test data
target=test_temp$TotalRevenueGenerated
test=subset(test_temp,select=-c(TotalRevenueGenerated))
pred=predict(LinReg_transform,test)
regr.eval(target,pred)
